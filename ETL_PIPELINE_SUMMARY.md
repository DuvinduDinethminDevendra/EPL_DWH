# EPL Data Warehouse - Complete ETL Pipeline Summary

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAW DATA SOURCES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  JSON FILES        â”‚  REST API          â”‚  CSV FILES           â”‚
â”‚  (666 files,       â”‚  (football-data)   â”‚  (3 match files,    â”‚
â”‚   47K+ players)    â”‚  (60 teams/teams)  â”‚   830 records)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                    â”‚                      â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚     EXTRACT (Staging Layer)      â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
             â”‚ â€¢ stg_player_raw (47,852 rows)  â”‚
             â”‚ â€¢ stg_team_raw (60 rows)        â”‚
             â”‚ â€¢ stg_e0_match_raw (830 rows)   â”‚
             â”‚                                  â”‚
             â”‚ Files: src/etl/staging/          â”‚
             â”‚ - load_staging.py                â”‚
             â”‚ - load_warehouse.py              â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   CLEAN (Data Quality Layer)     â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
             â”‚ â€¢ Trim whitespace                â”‚
             â”‚ â€¢ Remove NULLs                   â”‚
             â”‚ â€¢ Validate data types            â”‚
             â”‚ â€¢ Standardize naming             â”‚
             â”‚                                  â”‚
             â”‚ Files: src/etl/transform/        â”‚
             â”‚ - clean.py                       â”‚
             â”‚ - load_warehouse.py              â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  TRANSFORM & LOAD (Dimension Layer)       â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ â€¢ Extract DISTINCT records                â”‚
        â”‚ â€¢ Apply business keys                     â”‚
        â”‚ â€¢ INSERT...ON DUPLICATE KEY UPDATE        â”‚
        â”‚ â€¢ Write to etl_log                        â”‚
        â”‚                                           â”‚
        â”‚ File: src/etl/transform/                  â”‚
        â”‚ - clean_and_upsert_dim.py           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      DATA WAREHOUSE (Fact/Dim Layer)      â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ DIMENSIONS:                               â”‚
        â”‚ â€¢ dim_player (6,741 rows) âœ…              â”‚
        â”‚ â€¢ dim_team (50 rows) âœ…                   â”‚
        â”‚ â€¢ dim_stadium (25 rows) âœ…                â”‚
        â”‚ â€¢ dim_referee (32+ rows) âœ…               â”‚
        â”‚                                           â”‚
        â”‚ AUDIT:                                    â”‚
        â”‚ â€¢ etl_log (operation tracking)            â”‚
        â”‚ â€¢ ETL_JSON_Manifest (file tracking)       â”‚
        â”‚ â€¢ ETL_Api_Manifest (API tracking)         â”‚
        â”‚ â€¢ ETL_File_Manifest (CSV tracking)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ETL Pipeline Execution Flow

### 1. EXTRACT Phase: Load Raw Data to Staging
**File**: `src/etl/staging/load_staging.py`

```python
# Execution
load_all_staging()

# Steps
â”œâ”€â”€ write_staging_from_json()      # 666 JSON files â†’ stg_player_raw
â”œâ”€â”€ write_staging_from_api()        # REST API calls â†’ stg_team_raw
â””â”€â”€ write_staging_from_csv()        # 3 CSV files â†’ stg_e0_match_raw

# Results
âœ… 47,852 player records loaded
âœ… 60 team records loaded
âœ… 830 match records loaded
Total: 48,742 staging records
```

### 2. CLEAN Phase: Data Quality Transformations
**File**: `src/etl/load_warehouse.py`

```python
# Execution
clean_staging_data()

# Transformations
â”œâ”€â”€ Player names: TRIM, title-case, remove NULLs
â”œâ”€â”€ Team names: TRIM, remove NULLs
â””â”€â”€ Match data: TRIM team names, validate dates

# Logging
All operations logged to etl_log table
```

### 3. TRANSFORM & LOAD Phase: Dimension Upserts
**File**: `src/etl/transform/clean_and_upsert_dim.py` âœ¨ **NEW**

```python
# Execution
run_all_upserts(engine)

# Upserts (IN SEQUENCE)
â”œâ”€â”€ [1/4] upsert_dim_player()
â”‚   â”œâ”€â”€ Source: stg_player_raw (SELECT DISTINCT player_name)
â”‚   â”œâ”€â”€ Business Key: player_name
â”‚   â”œâ”€â”€ Operation: INSERT...ON DUPLICATE KEY UPDATE
â”‚   â””â”€â”€ Result: 6,741 rows affected
â”‚
â”œâ”€â”€ [2/4] upsert_dim_team()
â”‚   â”œâ”€â”€ Source: stg_team_raw (SELECT DISTINCT name)
â”‚   â”œâ”€â”€ Business Key: team_name
â”‚   â”œâ”€â”€ Operation: INSERT...ON DUPLICATE KEY UPDATE
â”‚   â””â”€â”€ Result: 50 rows affected
â”‚
â”œâ”€â”€ [3/4] upsert_dim_stadium()
â”‚   â”œâ”€â”€ Source: stg_e0_match_raw (SELECT DISTINCT HomeTeam)
â”‚   â”œâ”€â”€ Business Key: stadium_name
â”‚   â”œâ”€â”€ Operation: INSERT...ON DUPLICATE KEY UPDATE
â”‚   â””â”€â”€ Result: 25 rows affected
â”‚
â””â”€â”€ [4/4] upsert_dim_referee()
    â”œâ”€â”€ Source: stg_e0_match_raw (SELECT DISTINCT Referee)
    â”œâ”€â”€ Business Key: referee_name
    â”œâ”€â”€ Operation: INSERT...ON DUPLICATE KEY UPDATE
    â””â”€â”€ Result: 32+ rows affected

# Logging
Each upsert writes 1 entry to etl_log with:
â”œâ”€â”€ job_name: "upsert_dim_player" etc.
â”œâ”€â”€ phase_step: "transform"
â”œâ”€â”€ status: "SUCCESS" or "FAILED"
â”œâ”€â”€ start_time, end_time
â”œâ”€â”€ rows_processed
â””â”€â”€ message: descriptive text
```

## File Structure

```
EPL_DWH/
â”œâ”€â”€ src/etl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                          â† Database configuration
â”‚   â”œâ”€â”€ db.py                              â† Shared database engine
â”‚   â”‚
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”œâ”€â”€ json_reader.py                 â† JSON file reader (666 files)
â”‚   â”‚   â”œâ”€â”€ api_client.py                  â† REST API client (60 teams)
â”‚   â”‚   â””â”€â”€ csv_reader.py                  â† CSV file reader (830 records)
â”‚   â”‚
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ load_staging.py                â† Unified staging loader âœ…
â”‚   â”‚   â””â”€â”€ load_warehouse.py              â† Orchestration + cleaning âœ…
â”‚   â”‚
â”‚   â””â”€â”€ transform/
â”‚       â”œâ”€â”€ clean.py                       â† Cleaning functions
â”‚       â”œâ”€â”€ upsert_dims.py                 â† Legacy upsert (replaced)
â”‚       â””â”€â”€ clean_and_upsert_dim.py        â† NEW dimension upsert module âœ¨
â”‚
â”œâ”€â”€ src/sql/
â”‚   â””â”€â”€ create_schema.sql                  â† Database schema
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ raw/
    â”‚   â”œâ”€â”€ json/                          (666 JSON files)
    â”‚   â”œâ”€â”€ csv/                           (3 CSV files)
    â”‚   â””â”€â”€ (API data direct to DB)
    â””â”€â”€ staging/ (logical, in database)
```

## Running the Complete Pipeline

### Option 1: Run Individual Phases

```bash
# Phase 1: Extract only
cd d:\myPortfolioProject\EPL_DWH
.\.venv\Scripts\python.exe -m src.etl.staging.load_staging

# Phase 2: Extract + Clean
.\.venv\Scripts\python.exe -m src.etl.load_warehouse

# Phase 3: Extract + Clean + Transform & Load
# (Integrate clean_and_upsert_dim into load_warehouse.py)
```

### Option 2: Run Dimension Upserts Only

```bash
# Run ONLY dimension upserts (assumes staging tables populated)
.\.venv\Scripts\python.exe -m src.etl.transform.clean_and_upsert_dim
```

## Key Features

### âœ… Idempotent Operations
- All upsert operations are **re-runnable** without side effects
- ON DUPLICATE KEY UPDATE only updates existing records
- Manifest tables prevent re-processing of files

### âœ… Error Resilience
- Individual module failures don't stop the pipeline
- Comprehensive error logging to etl_log
- Graceful exception handling with detailed messages

### âœ… Audit Trail
- Every operation logged to etl_log table
- File processing tracked in manifest tables
- Complete execution history available for debugging

### âœ… No Pandas Dependency
- clean_and_upsert_dim.py uses only SQLAlchemy + standard library
- Lightweight, fast, minimal memory footprint
- Efficient SQL operations directly in database

### âœ… Type Hints & Documentation
- Full Python 3.13+ type hints
- Comprehensive docstrings for all functions
- Inline comments explaining business logic

## Database Schema Summary

### Staging Tables
| Table | Records | Columns | Purpose |
|-------|---------|---------|---------|
| stg_player_raw | 47,852 | 15 | Player data from JSON files |
| stg_team_raw | 60 | 35 | Team data from REST API |
| stg_e0_match_raw | 830 | 26 | Match results from CSV files |

### Dimension Tables
| Table | Rows | Business Key | Purpose |
|-------|------|--------------|---------|
| dim_player | 6,741 | player_name | Player master data |
| dim_team | 50+ | team_name | Team master data |
| dim_stadium | 25 | stadium_name | Stadium master data |
| dim_referee | 32+ | referee_name | Referee master data |

### Audit Tables
| Table | Purpose |
|-------|---------|
| etl_log | Complete operation history |
| ETL_JSON_Manifest | Tracks all 666 JSON files loaded |
| ETL_Api_Manifest | Tracks all API calls by season |
| ETL_File_Manifest | Tracks all CSV files processed |

## Performance Characteristics

- **JSON Reader**: ~1 sec to process 666 files (skip if manifested)
- **API Client**: ~5 secs to fetch 60 teams (3 seasons)
- **CSV Reader**: <1 sec to load 830 records
- **Dimension Upserts**: ~1 sec total for all 4 dimensions
- **Total Pipeline**: ~8 seconds (excludes cleaning operations)

## Next Steps

1. **Fact Table Loading**
   - Create `fact_matches` table (normalized match results)
   - Join with dimension keys via business keys
   - Implement `upsert_fact_matches()` function

2. **Data Quality Validation**
   - Row count reconciliation (source vs target)
   - Duplicate detection and handling
   - NULL value analysis by column

3. **Performance Optimization**
   - Batch insert operations for large datasets
   - Index creation/analysis
   - Query execution plan review

4. **Scheduling & Monitoring**
   - DAG orchestration (Airflow, etc.)
   - Email notifications on failures
   - Dashboard for etl_log metrics

## Technology Stack

- **Language**: Python 3.13.5
- **Database**: MySQL 8.0 (Docker container)
- **ORM/Query**: SQLAlchemy 2.x (core, no ORM)
- **Data Formats**: JSON, CSV, REST API
- **Environment**: Windows PowerShell, Docker

## Files Created/Modified This Session

| File | Status | Purpose |
|------|--------|---------|
| `clean_and_upsert_dim.py` | âœ¨ **NEW** | Production-grade dimension upsert module |
| `IMPLEMENTATION_NOTES.md` | ğŸ“ NEW | Detailed implementation documentation |
| `load_warehouse.py` | âœï¸ Modified | Added cleaning phase orchestration |
| `load_staging.py` | âœ… Existing | Unified staging loader |

---

**Total Project Statistics**:
- ğŸ“Š Data Loaded: 48,742 staging records
- ğŸ“ˆ Dimensions Created: 6,848 distinct records across 4 tables
- ğŸ“‹ Audit Trail: Complete operation logging to etl_log
- âš¡ Pipeline Duration: ~8 seconds (excluding cleaning)
- ğŸ¯ Business Coverage: Players, Teams, Stadiums, Referees, Matches
