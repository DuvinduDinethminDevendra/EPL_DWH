# EPL DWH - Quick Reference Guide

**Last Updated**: November 1, 2025  
**Status**: âœ… **PRODUCTION READY**

---

## ğŸš€ Essential Commands

### Run Full ETL Pipeline (All-in-One)
```powershell
python -m src.etl.main --full-etl-and-facts
```
**Duration**: ~10 minutes  
**Output**: Schema + dimensions + 2.6M+ events + cleanup

### Fast Testing with Limited Data
```powershell
# Test with 10 files (~40 seconds, 36K events)
python -m src.etl.main --full-etl-and-facts --limit-data 10

# Test with 30 files (~2 minutes, 106K events)
python -m src.etl.main --full-etl-and-facts --limit-data 30
```

### Step-by-Step (If Needed)
```powershell
# Step 1: Extract & load dimensions
python -m src.etl.main --full-etl

# Step 2: Load facts from staging
python -m src.etl.main --load-fact-tables

# Check database connection
python -m src.etl.main --test-db
```

---

## ğŸ“Š Current Data Status

### âœ… Data Loaded
```
âœ… 830 CSV matches (2023-2025 seasons)
âœ… 2,675,770 StatsBomb event records
âœ… 380 match-to-event mappings
âœ… 6,847 unique players
âœ… 25 EPL teams
âœ… Full dimensional model
```

### âœ… Data Quality
```
âœ… Zero foreign key violations
âœ… All events have calendar dates
âœ… Complete referential integrity
âœ… Sentinel records for unknowns (-1, 6808)
```

### ğŸ“ˆ Performance Metrics
- **Load Time**: ~10 minutes (full pipeline)
- **Event Processing**: 2.6M+ events
- **Fact Tables**: 3 (match, events, player_stats)
- **Dimensions**: 6 (shared across all facts)
- **Total Tables**: 21 (facts + dims + mappings + audit)

---

## ğŸ“‹ Table Overview

### Fact Tables
| Table | Rows | Purpose |
|-------|------|---------|
| fact_match | 830 | Match summaries with scores/stats |
| fact_match_events | 2.6M+ | Detailed event-by-event breakdown |
| fact_player_stats | 1,600 | Player performance per match |

### Dimension Tables
| Table | Rows | Purpose |
|-------|------|---------|
| dim_date | 17.5K | Calendar dates (1990-2025) |
| dim_team | 25 | EPL teams + sentinel |
| dim_player | 6,847 | All players + sentinels |
| dim_referee | 32 | Match officials |
| dim_stadium | 25 | EPL venues |
| dim_season | 7 | Season definitions |

### Mapping/Bridge Tables
| Table | Rows | Purpose |
|-------|------|---------|
| dim_team_mapping | ~40 | StatsBomb IDs â†’ DWH IDs |
| dim_match_mapping | 380 | Match ID translation |

### Audit/Metadata Tables
| Table | Purpose |
|-------|---------|
| ETL_Log | All pipeline operations |
| ETL_Events_Manifest | Event file tracking (deduplication) |
| ETL_File_Manifest | CSV file tracking |
| ETL_Api_Manifest | API call tracking |
| ETL_Excel_Manifest | Excel file tracking |

---

## âœ… Quick Setup

### 1. Prerequisites
```powershell
# Check Python
python --version

# Check Docker (MySQL running)
docker ps | findstr epl_mysql

# Activate environment
.\.venv\Scripts\Activate.ps1
```

### 2. Run Full Pipeline
```powershell
python -m src.etl.main --full-etl-and-facts
```

### 3. Verify Results
```powershell
python -c "
from src.etl.db import get_engine
from sqlalchemy import text

engine = get_engine()
with engine.connect() as conn:
    # Check fact tables
    facts = conn.execute(text('SELECT COUNT(*) FROM fact_match_events'))
    print(f'Events loaded: {facts.fetchone()[0]:,}')
    
    # Check dimensions
    players = conn.execute(text('SELECT COUNT(*) FROM dim_player'))
    print(f'Players: {players.fetchone()[0]:,}')
"
```

---

## ğŸ” Verify Data Loading

### Check All Tables
```sql
-- Check row counts
SELECT 'fact_match' as tbl, COUNT(*) as cnt FROM fact_match
UNION ALL SELECT 'fact_match_events', COUNT(*) FROM fact_match_events
UNION ALL SELECT 'dim_team', COUNT(*) FROM dim_team
UNION ALL SELECT 'dim_player', COUNT(*) FROM dim_player;

-- Check mappings populated
SELECT 'dim_match_mapping', COUNT(*) FROM dim_match_mapping
UNION ALL SELECT 'dim_team_mapping', COUNT(*) FROM dim_team_mapping;

-- Check no duplicates
SELECT COUNT(*) as duplicate_count
FROM (
    SELECT event_id, COUNT(*) 
    FROM fact_match_events 
    GROUP BY event_id 
    HAVING COUNT(*) > 1
) t;
```

### Check Manifest (Deduplication)
```sql
-- Verify manifest tracking
SELECT COUNT(*) FROM ETL_Events_Manifest;  -- Should be 380 (one per file)

-- See latest load status
SELECT job_name, status, COUNT(*) 
FROM ETL_Log 
GROUP BY job_name, status 
ORDER BY job_name;
```

---

## ğŸ¯ Schema Pattern

Your DWH is a **Fact Constellation Schema** with:

```
3 Fact Tables â”€â”€â”€â”€â”€â”€â”€â”€ Share â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6 Dimensions
â”œâ”€ fact_match                          â”œâ”€ dim_date
â”œâ”€ fact_match_events                   â”œâ”€ dim_team
â””â”€ fact_player_stats                   â”œâ”€ dim_player
                                       â”œâ”€ dim_referee
                                       â”œâ”€ dim_stadium
                                       â””â”€ dim_season
```

**Key Benefits**:
- Multiple analytical perspectives (match/event/player levels)
- No dimension duplication (single source of truth)
- Efficient multi-source integration
- Complete audit trail and deduplication

See `FACT_CONSTELLATION_QUICK_REFERENCE.md` for full schema pattern details.

---

## ğŸ“ Project Structure

```
EPL_DWH/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ main.py              â† Entry point (CLI commands)
â”‚   â”‚   â”œâ”€â”€ extract/             â† Data extraction
â”‚   â”‚   â”œâ”€â”€ transform/           â† Data transformation
â”‚   â”‚   â”œâ”€â”€ load_warehouse.py    â† Dimension loading
â”‚   â”‚   â””â”€â”€ staging/             â† Staging tables
â”‚   â”œâ”€â”€ sql/                     â† SQL scripts
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     â† Source files
â”‚   â”œâ”€â”€ staging/
â”‚   â””â”€â”€ fbref_html/
â”œâ”€â”€ docker-compose.yml           â† MySQL config
â”œâ”€â”€ requirements.txt             â† Python dependencies
â””â”€â”€ README.md                    â† Main documentation
```

---

## ğŸ”§ Common Tasks

### Reset Database (Full Reload)
```powershell
# Stop and remove old container
docker-compose down

# Clean up data (if needed)
# Remove mysql_data/ folder

# Start fresh
docker-compose up -d

# Run full pipeline
python -m src.etl.main --full-etl-and-facts
```

### Run Only Staging (Data Extraction)
```powershell
python -m src.etl.main --staging
```

### Run Only Dimensions (Schema + Transform)
```powershell
python -m src.etl.main --warehouse
```

### Check Specific Table
```powershell
python -c "
from src.etl.db import get_engine
from sqlalchemy import text

engine = get_engine()
with engine.connect() as conn:
    result = conn.execute(text('SELECT * FROM fact_match LIMIT 5'))
    for row in result:
        print(row)
"
```

---

## âš ï¸ Troubleshooting

### Database Connection Failed
```powershell
# Check if Docker container is running
docker ps | findstr mysql

# Check logs
docker logs epl_mysql

# Restart container
docker-compose restart
```

### Out of Memory During Load
```powershell
# Use limit-data to load in batches
python -m src.etl.main --full-etl-and-facts --limit-data 10

# Then load more
python -m src.etl.main --full-etl-and-facts --limit-data 20
```

### FK Constraint Violations
```powershell
# Check if mappings are populated
python -c "
from src.etl.db import get_engine
from sqlalchemy import text

engine = get_engine()
with engine.connect() as conn:
    cnt = conn.execute(text('SELECT COUNT(*) FROM dim_match_mapping'))
    print(f'Mappings: {cnt.fetchone()[0]}')
"
```

### ETL Log Shows Errors
```sql
-- Check latest errors
SELECT job_name, status, message 
FROM ETL_Log 
WHERE status IN ('FAILED', 'WARNING')
ORDER BY log_id DESC 
LIMIT 10;
```

---

## ğŸ“š Documentation Files

| File | Purpose |
|------|---------|
| **README.md** | Main project overview |
| **ETL_PIPELINE_GUIDE.md** | Detailed ETL process explanation |
| **DATABASE_RELATIONSHIPS_ER_DIAGRAM.md** | Schema relationships & ER diagram |
| **FACT_CONSTELLATION_QUICK_REFERENCE.md** | Schema pattern explanation |
| **MAINTENANCE.md** | Operations & maintenance guide |
| **This File** | Quick commands & reference |

---

## ğŸš€ Next Steps

1. **Run the Pipeline**: `python -m src.etl.main --full-etl-and-facts`
2. **Verify Data**: Check row counts and relationships
3. **Read the Guides**: 
   - `README.md` for overview
   - `ETL_PIPELINE_GUIDE.md` for details
4. **Query the Data**: Write analytics queries
5. **Reference**: Use `FACT_CONSTELLATION_QUICK_REFERENCE.md` for schema understanding

---

**Status**: âœ… Ready for production use  
**Last Tested**: November 1, 2025  
**Data Volume**: 2.6M+ events loaded successfully

